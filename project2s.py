# -*- coding: utf-8 -*-
"""project2s.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CsAup7jSu3GgCXPVTzimhrkoHRY61uac

About the Dataset:
1.id: unique id for a news article
2.title: the title of a news article
3.author: author of the news article
4.text: the text of the article; could be incomplete
5.label: a label that marks whether the news article is real or fake:
          1: Fake news
          0: real News
"""

import matplotlib.pyplot as plt
import seaborn as sns
import scipy.sparse as sp
import numpy as np
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import nltk
nltk.download('stopwords')

# printing the stopwords in English
print(stopwords.words('english'))

# loading the dataset to a pandas DataFrame
my_dataset = pd.read_csv('/content/sample_data/train.csv')

my_dataset.shape

# print the first 5 rows of the dataframe
my_dataset.head()

# titles = my_dataset["title"]
# labels = my_dataset["label"]

# x=[]
# y=[]

x=list(titles)
y=list(labels)

"""**Data Cleaning**"""

# counting the number of missing values in the dataset
my_dataset.isnull().sum()

# replacing the null values with empty string
my_dataset = my_dataset.fillna('')

"""Data Exploration"""

my_dataset

plot = sns.boxplot(x='label',y="id",data=my_dataset)

#print(my_dataset.groupby("label")('1').count())
# data.groupby([‘target’])[‘text’].count().plot(kind=”bar”)
# plt.show()

# merging the author name and news title
my_dataset['content'] = my_dataset['author']+' '+my_dataset['title']

print(my_dataset['content'])

# separating the data & label
X = my_dataset.drop(columns='label', axis=1)
Y = my_dataset['label']

print(X)

print(Y)

port_stem = PorterStemmer()

def stemming(content):
    stemmed_content = re.sub('[^a-zA-Z]',' ',content)
    stemmed_content = stemmed_content.lower()
    stemmed_content = stemmed_content.split()
    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
    stemmed_content = ' '.join(stemmed_content)
    return stemmed_content

my_dataset['content'] = my_dataset['content'].apply(stemming)

print(my_dataset['content'])

#separating the data and label
X = my_dataset['content'].values
Y = my_dataset['label'].values

print(X)

print(Y)

"""Data Analysis and Visualization"""



#Analyzing the frequency of news with word cloud(Data Visualization Technique)
from wordcloud import WordCloud
fake_data = my_dataset[my_dataset["label"] == "1"]
all_words = ' '.join([text for text in fake_data.text])
counts = my_dataset['label'].value_counts()
counts.index = counts.index.map(str)
wordcloud = WordCloud().generate_from_frequencies(counts)
plt.figure(figsize=(10,7))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

#Analysing number of false and true news
c_real = 0
c_false = 0
for i in Y:
  if Y[i]==0:
    c_real=c_real+1
  else :
    c_false = c_false+1
print("Number of real news: "+ str(c_real))
print("Number of false news: "+ str(c_false))

#Visualizing percentage of fake and real news by cateogery in train file
label_size =[c_real,c_false]
plt.pie(label_size,explode=[0.1,0.1],colors=['red','green'],shadow=True,labels=['False','Real'],autopct='%1.1f%%')

# width =0.3
# plt.bar(np.arange(c_real, c_real, width=width)
# plt.bar(np.arange(len(Y))+ width, Y, width=width)
# plt.show()

Y.shape
print(Y)

# converting the textual data to numerical data 
vectorizer = TfidfVectorizer()
vectorizer.fit(X)

X = vectorizer.transform(X)

print(X)

type(X)

"""splitting the data set in to training and test data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, stratify=Y, random_state=2)

print(X_train)

print(X_test)

X_test.data

type(X_test)

X_test.shape

type(X_test.shape)

i = X_test.shape[0]

j= i-1
print(j)

X_test.nnz

print(Y_train)
print(len(Y_train))

print(Y_test)

#visualizing test data and train data
label_size =[len(Y_train),len(Y_test)]
plt.pie(label_size,explode=[0.1,0.1],colors=['Yellow','blue'],shadow=True,labels=['Train','Test'],autopct='%1.1f%%')

model = LogisticRegression()

model.fit(X_train, Y_train)

# accuracy score on the training data
X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy)

print("Enter the val needs to be tested to know whether the news is fake or not")
print("The value must be between 0 and "+ str(j) + " as this is the range of test data")
val = int(input())

try:
    X_new = X_test[val]
    #X_new = X_test[3]
    prediction = model.predict(X_new)
    view =model.predict_proba(X_new)
    print(prediction)
    print(view)
    if (prediction[0]==0):
      print('The news is Real')
    else:
      print('The news is Fake')
except:
    print("Out of range")

try:
 print(Y_test[val])
except:
  print("Out of range")